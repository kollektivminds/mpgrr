{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MAPRR Textual Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "import logging\n",
    "from threading import *\n",
    "import concurrent\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd \n",
    "import re\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "from natasha import (\n",
    "    Segmenter, \n",
    "    MorphVocab, \n",
    "    NewsEmbedding, \n",
    "    NewsMorphTagger, \n",
    "    NewsSyntaxParser, \n",
    "    NewsNERTagger, \n",
    "    PER, \n",
    "    NamesExtractor, \n",
    "    Doc)\n",
    "from razdel import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='maprr_out.log', encoding='utf-8', format='%(asctime)s %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_cols = ['title_ru', 'genre', 'text', 'title_en', '1st_line', 'author', 'comp_date', 'comp_loc', 'pub_src', '1st_pub', 'pub_year', 'pub_loc']\n",
    "a_cols = ['name', 'birth', 'death', 'a_type', 'sex', 'occs', 'fam_soc_str', 'lit_affil', 'pol_affil', 'corp_type', 'corp_affil']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'https://maprr.iath.virginia.edu/'\n",
    "max_threads = 3\n",
    "tables = {\n",
    "    'agents/': 332, \n",
    "    'works/': 655, \n",
    "    'place_based_concepts/': 316, \n",
    "    'locations/': 380, \n",
    "    'multivalent_markers/': 449\n",
    "}\n",
    "tables = {\n",
    "    'agents/': 3,\n",
    "    'works/': 6,\n",
    "    'place_based_concepts/': 3, \n",
    "    'locations/': 3,\n",
    "    'multivalent_markers/': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "allURLs = [domain+item[0]+str(i) for item in tables.items() for i in range(1, item[1]+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb+') as f:\n",
    "        pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls_to_visit = [] \n",
    "for t, i in list(tables.items())[:2]: \n",
    "    for j in range(0,i+1): \n",
    "        urls_to_visit.append(domain+t+str(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class maprr: \n",
    "    \n",
    "    def __init__(self): \n",
    "        self.Wsoup = {} \n",
    "        self.Asoup = {}\n",
    "        self.Ws = {}\n",
    "        self.As = {}\n",
    "    \n",
    "    def get_htmlA(self): \n",
    "        \"\"\"This function uses the list of Agent IDs from the tables dict above\n",
    "        and the grabs it using requests before putting the html reponse in Asoup\"\"\"\n",
    "        \n",
    "        # initialize list of pages that don't return 200\n",
    "        aberrantAs = []\n",
    "        # go through list of Agents from 1 to the number defined in tables\n",
    "        for i in range(1, (list(tables.values())[0]+1)):\n",
    "        #for i in range(1, 11):\n",
    "            # make url\n",
    "            url = domain+list(tables.keys())[0]+str(i) \n",
    "            # initialize connection to url \n",
    "            with requests.get(url, verify=False) as r: \n",
    "                # log status code \n",
    "                logging.info(f\"A{i} status code: {r.status_code}\")\n",
    "                # if connection is successful\n",
    "                if r.status_code == 200: \n",
    "                    # make soup from html\n",
    "                    s = BeautifulSoup(r.content, 'html.parser') \n",
    "                    # add to list of A soups\n",
    "                    self.Asoup.update({i:s})\n",
    "                else: \n",
    "                    # if connection is not successful, add to list\n",
    "                    aberrantAs.append((i, r.status_code))\n",
    "                    pass\n",
    "                # wait a hot second or the server gets >:( \n",
    "                time.sleep(.1)\n",
    "        # report list of A errors if there is one (there always is)\n",
    "        if len(aberrantAs) > 0: \n",
    "            print(f\"Aberrant agent pages are #s {aberrantAs}\")\n",
    "        else: \n",
    "            print(f\"There were no aberrant agent pages!\")\n",
    "\n",
    "    def get_htmlW(self): \n",
    "        \"\"\"This function uses the list of Work IDs from the tables dict above\n",
    "        and the grabs it using requests before putting the html reponse in Wsoup\"\"\"\n",
    "        \n",
    "        # initialize list of pages that don't return 200\n",
    "        aberrantWs = []\n",
    "        # go through list of Words from 1 to the number defined in tables\n",
    "        for i in range(1, (list(tables.values())[1]+1)):\n",
    "        #for i in range(1, 11):\n",
    "            # make url\n",
    "            url = domain+list(tables.keys())[1]+str(i)\n",
    "            # initialize connection to url \n",
    "            with requests.get(url, verify=False) as r: \n",
    "                # log status code\n",
    "                logging.info(f\"W{i} status code: {r.status_code}\")\n",
    "                # if connection is successful\n",
    "                if r.status_code == 200: \n",
    "                    # make soup from html\n",
    "                    s = BeautifulSoup(r.content, 'html.parser')\n",
    "                    # add to list of A soups\n",
    "                    self.Wsoup.update({i:s})\n",
    "                else: \n",
    "                    # if connection is not successful, add to list\n",
    "                    aberrantWs.append((i, r.status_code))\n",
    "                    pass\n",
    "                # wait a hot second or the server gets >:( \n",
    "                time.sleep(.1)\n",
    "        # report list of A errors if there is one (there always is)\n",
    "        if len(aberrantWs) > 0: \n",
    "            print(f\"Aberrant work pages are #s {aberrantWs}\")\n",
    "        else: \n",
    "            print(f\"There were no aberrant work pages!\")\n",
    "\n",
    "    def parseWs(self, html): \n",
    "        \"\"\"This function retrieves Work info from the HTML provided\"\"\"\n",
    "        \n",
    "        # extract things we will need\n",
    "        content = html.find('div', {'class':'col-md-9 fixed-height'})\n",
    "        # try to locate author text of Work\n",
    "        try: \n",
    "            author = content.div.h3.text\n",
    "        except: \n",
    "            author = \"unknown\"\n",
    "        # try to locate title text of Work\n",
    "        try: \n",
    "            title = content.div.h4.text\n",
    "        except: \n",
    "            title = \"untitled\"\n",
    "        # try for both stanza and para text because they return [] \n",
    "        stanza_text = content.find_all('p',{'class':'stanza'})\n",
    "        prose_text = content.find_all('p',{'class':'text'})\n",
    "        # decide which text to use based on length \n",
    "        if len(prose_text) > len(stanza_text): \n",
    "            text = prose_text\n",
    "            genre = 'prose'\n",
    "        else: \n",
    "            text = stanza_text\n",
    "            genre = 'poetry'\n",
    "        # get actual text \n",
    "        Wtext = [x.text.replace('\\n','').strip() for x in text]\n",
    "        # make list of keys of Work types\n",
    "        typeKeys = [x.text[:-1] for x in html.find('div', {'class':'card-body'}).find_all('h4')]\n",
    "        # make list of values of Work types \n",
    "        typeVals = [x.text for x in html.find('div', {'class':'card-body'}).find_all('p')]\n",
    "        # make dictionary of keys:values from above\n",
    "        typeDict = dict(zip(typeKeys, typeVals))\n",
    "        # initialize sub dictionary of Work\n",
    "        Wdict = {'title': title, \n",
    "                   'genre': genre,\n",
    "                   'text': Wtext} \n",
    "        # add type keys and values to complete sub dictionary of Work\n",
    "        Wdict.update(typeDict) \n",
    "        # return Work dict to be made into a DataFrame row\n",
    "        return Wdict\n",
    "\n",
    "    def parseAs(self, html): \n",
    "        \"\"\"This function retrieves Agent info from the HTML provided\"\"\"\n",
    "        \n",
    "        # get Agent's name\n",
    "        name = html.find('div', {'class': 'card scrollable'}).h2.text \n",
    "        # get Agent's birth- and deathdates \n",
    "        bdate, ddate = html.find('div', {'class': 'card scrollable'}).span.text.split(' - ') \n",
    "        # initialize dictionary of Agent\n",
    "        Adict = {'name': name, 'birth': bdate, 'death': ddate}\n",
    "        # make list of type keys\n",
    "        typeKeys = [x.h4.text.lower().replace(' ','_') for x in html.find_all('div', {'class': 'col-md-4'})]\n",
    "        # initialize list of type values\n",
    "        typeVals = []\n",
    "        # add type values to list if found or default to 'unknown' (though some real values are also 'unknown')\n",
    "        for typ in html.find_all('div', {'class': 'col-md-4'}): \n",
    "            try: \n",
    "                typeVals.append(typ.p or typ.div.span.text)\n",
    "            except: \n",
    "                typeVals.append(\"unknown\")\n",
    "        # for some reason keys are at different levels and require '.text.' attribute but of course some don't\n",
    "        typeVals = [x.text if not isinstance(x, str) else x for x in typeVals]\n",
    "        # make dictionary of keys:values from above\n",
    "        typeDict = dict(zip(typeKeys, typeVals))\n",
    "        # add type keys and values to complete sub dictionary of Work\n",
    "        Adict.update(typeDict) \n",
    "        # return Agent dict to be made into a DataFrame row \n",
    "        return Adict\n",
    "    \n",
    "    def get_single(self, cat, id_num): \n",
    "        \"\"\"This function combines the functions above and returns the DataFrame row\"\"\" \n",
    "        \n",
    "        # make url from parameters and initialize request \n",
    "        with requests.get(domain+cat+'s/'+str(id_num), verify=False) as r: \n",
    "            # double check the URL is correct\n",
    "            print(r.url)\n",
    "            # check status code\n",
    "            print(f\"{cat+str(id_num)} status code: {r.status_code}\")\n",
    "            # if connection is successful\n",
    "            if r.status_code == 200: \n",
    "                # make soup from html content \n",
    "                s = BeautifulSoup(r.content, 'html.parser')\n",
    "                # sort by FOO type \n",
    "                if cat.lower() == 'work': \n",
    "                    # make dictionary if Work with parseWs function\n",
    "                    SubDict = {id_num:self.parseWs(s)}\n",
    "                elif cat.lower() == 'agent': \n",
    "                    # make dictionary if Agent with parseAs function\n",
    "                    SubDict = {id_num:self.parseAs(s)} \n",
    "                else: \n",
    "                    print(\"You need a category: 'work' or 'agent'...\") \n",
    "                # make DataFrame row from dictionary\n",
    "                singleDf = pd.DataFrame.from_dict(newSubDict, orient='index')\n",
    "                # return DataFrame row\n",
    "                return singleDf\n",
    "            # return status code if connection is unsuccessful\n",
    "            else: \n",
    "                print(f\"Error: {r.status_code}\") \n",
    "                \n",
    "    def save_obj(self, obj, name):\n",
    "        with open(name + '.pkl', 'wb+') as f:\n",
    "            pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def load_obj(self, name):\n",
    "        with open(name + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "\n",
    "    def run(self): \n",
    "        \"\"\"This function runs retrieval and parsing using the functions above, creates DataFrames, and persists them (JSON)\"\"\"\n",
    "        \n",
    "        logging.info(f\"Getting As and Ws\")\n",
    "        print(f\"Getting As\")\n",
    "        # get starting time for A retrieval\n",
    "        at1 = time.time()\n",
    "        # retrieve As\n",
    "        self.get_htmlA() \n",
    "        # get finishing time for A retrieval\n",
    "        at2 = time.time()\n",
    "        # display run times for A retrieval \n",
    "        logging.info(f\"Got As in {round((at2-at1), 3)} sec ({round(((at2-at1)/list(tables.values())[0]), 5)} sec/ea.)\")\n",
    "        \n",
    "        print(f\"Getting Ws\")\n",
    "        # get starting time for W retrieval\n",
    "        wt1 = time.time()\n",
    "        # retrieve As\n",
    "        self.get_htmlW() \n",
    "        # get finishing time for W retrieval\n",
    "        wt2 = time.time()\n",
    "        # display run times for W retrieval \n",
    "        print(f\"Got Ws in {round((wt2-wt1), 3)} sec ({round(((wt2-wt1)/list(tables.values())[1]), 5)} sec/ea.)\")\n",
    "        logging.info(f\"Got Ws in {round((wt2-wt1), 3)} sec ({round(((wt2-wt1)/list(tables.values())[1]), 5)} sec/ea.)\")\n",
    "        \n",
    "        logging.info(f\"Done getting As and Ws\")\n",
    "        \n",
    "        self.save_obj(self.Asoup, 'Asoup')\n",
    "        self.save_obj(self.Wsoup, 'Wsoup')\n",
    "        \n",
    "        logging.info(f\"Parsing As and Ws\")\n",
    "        \n",
    "        logging.info(f\"Parsing As\")\n",
    "        print(f\"Parsing As\")\n",
    "        # get starting time for A parsing\n",
    "        pa1 = time.time()\n",
    "        # parse Agent HTML instances\n",
    "        self.As = {k: self.parseAs(v) for k, v in self.Asoup.items()}\n",
    "        # get finishing time for A parsing\n",
    "        pa2 = time.time()\n",
    "        # display run times for A parsing \n",
    "        logging.info(f\"Parsed As in {round((pa2-pa1), 3)} sec ({round((pa2-pa1)/len(list(self.Asoup.items())), 5)} sec/ea.)\")\n",
    "        \n",
    "        print(f\"Parsing Ws\")\n",
    "        # get starting time for W parsing\n",
    "        pw1 = time.time()\n",
    "        # parse Work HTML instances \n",
    "        self.Ws = {k: self.parseWs(v) for k, v in self.Wsoup.items()}\n",
    "        # get finishing time for W parsing\n",
    "        pw2 = time.time() \n",
    "        # display run times for W parsing \n",
    "        print(f\"Parsed Ws in {round((pw2-pw1), 3)} sec ({round((pw2-pw1)/len(list(self.Wsoup.items())), 5)} sec/ea.)\")\n",
    "        logging.info(f\"Parsed Ws in {round((pw2-pw1), 3)} sec ({round((pw2-pw1)/len(list(self.Wsoup.items())), 5)} sec/ea.)\")\n",
    "        logging.info(f\"Done parsing As and Ws\")\n",
    "        \n",
    "        logging.info(f\"Making dataframes\")\n",
    "        print(f\"Making AsDf\")\n",
    "        # create DataFrame of Agent dictionaries\n",
    "        AsDf = pd.DataFrame.from_dict(self.As, orient='index')\n",
    "        print(f\"Making WsDf\")\n",
    "        # create DataFrame of Work dictionaries\n",
    "        WsDf = pd.DataFrame.from_dict(self.Ws, orient='index')  \n",
    "        logging.info(f\"Done making dataframes\")\n",
    "        \n",
    "        logging.info(f\"Writing to json\")\n",
    "        # write Work DataFrame to JSON\n",
    "        WsDf.to_json('WsDf.json')\n",
    "        # write Agent DataFrame to JSON\n",
    "        AsDf.to_json('AsDf.json')\n",
    "        logging.info(f\"Done writing to json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_status(urls): \n",
    "    aberrantURLs = []\n",
    "    logging.info(f\"Checking status of URLs\")\n",
    "    for url in urls: \n",
    "        logging.info(f\"Trying {url}\")\n",
    "        with requests.get(url, verify=False) as r: \n",
    "            if r.status_code == 200: \n",
    "                #logging.info(f\"{url} successful\")\n",
    "                pass\n",
    "            else: \n",
    "                logging.info(f\"{url}: {r.status_code}\")\n",
    "                aberrantURLs.append({url: r.status_code})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "class MAPRR: \n",
    "    \n",
    "    def __init__(self): \n",
    "        self.urls_to_visit = []\n",
    "        self.aberrantAs = []\n",
    "        self.aberrantWs = []\n",
    "        self.soups = {}\n",
    "        self.Wsoup = {} \n",
    "        self.Asoup = {}\n",
    "        self.Ws = {}\n",
    "        self.As = {}\n",
    "    \n",
    "    def get_html(self, url): \n",
    "        url_format = 'https://mpgrr.herokuapp.com/(\\w+)/(\\d{1,3})'\n",
    "        url_match = re.match(url_format, url)\n",
    "        fco_type = url_match.group(1)\n",
    "        id_num = url_match.group(2)\n",
    "        headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',\n",
    "    }\n",
    "        with requests.get(url, headers=headers) as r: \n",
    "            logging.info(f\"{fco_type}/{id_num} status code: {r.status_code}\")\n",
    "            if r.status_code == 200: \n",
    "                s = BeautifulSoup(r.content, 'html.parser')\n",
    "                #self.soups.update({id_num:s})\n",
    "                if fco_type == 'agents': \n",
    "                    self.Asoup.update({id_num:s})\n",
    "                elif fco_type == 'works': \n",
    "                    self.Wsoup.update({id_num:s})\n",
    "            else: \n",
    "                if fco_type == 'agents': \n",
    "                    self.aberrantAs.append({'A'+str(id_num): r.status_code})\n",
    "                elif fco_type == 'works': \n",
    "                    self.aberrantWs.append({'W'+str(id_num): r.status_code})\n",
    "                pass\n",
    "        time.sleep(.5)\n",
    "        \n",
    "    def parse_html(self, html): \n",
    "        if 'works' in list(html.body.attrs.values())[0]: \n",
    "            content = html.find('div', {'class':'col-md-9 fixed-height'})\n",
    "            try: \n",
    "                author = content.div.h3.text\n",
    "            except: \n",
    "                author = \"\"\n",
    "            try: \n",
    "                title = content.div.h4.text\n",
    "            except: \n",
    "                title = \"\"\n",
    "            stanza_text = content.find_all('p',{'class':'stanza'})\n",
    "            prose_text = content.find_all('p',{'class':'text'})\n",
    "            if len(stanza_text) > len(prose_text): \n",
    "                text = stanza_text\n",
    "            elif len(stanza_text) < len(prose_text): \n",
    "                text = prose_text\n",
    "            Wtext = [x.text.replace('\\n','').strip() for x in text]\n",
    "            metaKeys = [x.text[:-1] for x in html.find('div', {'class':'card-body'}).find_all('h4')]\n",
    "            metaVals = [x.text for x in html.find('div', {'class':'card-body'}).find_all('p')]\n",
    "            metaDict = dict(zip(metaKeys, metaVals))\n",
    "            subDict = {'title': title, \n",
    "                       'text': Wtext}\n",
    "\n",
    "            #self.Ws.update(subdict)\n",
    "            return subDict\n",
    "\n",
    "        elif 'agents' in list(html.body.attrs.values())[0]: \n",
    "            name = html.find('div', {'class': 'card scrollable'}).h2.text\n",
    "            bdate, ddate = html.find('div', {'class': 'card scrollable'}).span.text.split(' - ')\n",
    "            subDict = {'name': name, 'birth': bdate, 'death': ddate}\n",
    "            try: \n",
    "                typeKeys = [x.h4.text for x in html.find_all('div', {'class': 'col-md-4'})]\n",
    "                typeVals = [x.p or x.div.span.text for x in html.find_all('div', {'class': 'col-md-4'})]\n",
    "                typeVals[:2] = [x.text for x in typeVals[:2]]\n",
    "                typeDict = dict(zip(typeKeys, typeVals))\n",
    "                subDict.update(typeDict)\n",
    "            except: \n",
    "                pass\n",
    "\n",
    "            #self.As.update(subdict)\n",
    "            return subDict\n",
    "\n",
    "    def run(self): \n",
    "        for t, i in list(tables.items())[:2]: \n",
    "            for j in range(0,5): \n",
    "                self.urls_to_visit.append(domain+t+str(j))\n",
    "        \n",
    "        logging.info(f\"Getting As and Ws\")\n",
    "        for url in self.urls_to_visit: \n",
    "            self.get_html(url) \n",
    "        logging.info(f\"Done getting As and Ws\")\n",
    "        \n",
    "        logging.info(f\"Parsing As and Ws\")\n",
    "        print(f\"Parsing As\")\n",
    "        self.As = {k: self.parse_html(v) for k, v in self.Asoup.items()}\n",
    "        print(f\"Parsing Ws\")\n",
    "        self.Ws = {k: self.parse_html(v) for k, v in self.Wsoup.items()}        \n",
    "        logging.info(f\"Done parsing As and Ws\")\n",
    "        \n",
    "        logging.info(f\"Making dataframes\")\n",
    "        print(f\"Making AsDf\")\n",
    "        AsDf = pd.DataFrame.from_dict(self.As, orient='index')\n",
    "        print(f\"Making WsDf\")\n",
    "        WsDf = pd.DataFrame.from_dict(self.Ws, orient='index')        \n",
    "        logging.info(f\"Done making dataframes\")\n",
    "        \n",
    "        logging.info(f\"Writing to json\")\n",
    "        WsDf.to_json('WsDf.json')\n",
    "        AsDf.to_json('AsDf.json')\n",
    "        logging.info(f\"Done writing to json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ParallelMAPRR: \n",
    "    \n",
    "    global domain, tables, max_threads\n",
    "    \n",
    "    def __init__(self): \n",
    "        self.urls_to_visit = []\n",
    "        self.aberrantAs = []\n",
    "        self.aberrantWs = []\n",
    "        self.soups = {}\n",
    "        self.Wsoup = {} \n",
    "        self.Asoup = {}\n",
    "        self.Ws = {}\n",
    "        self.As = {}\n",
    "    \n",
    "    def get_html(self, url): \n",
    "        url_format = 'https://maprr.iath.virginia.edu/(\\w+)/(\\d{1,3})'\n",
    "        url_match = re.match(url_format, url)\n",
    "        fco_type = url_match.group(1)\n",
    "        id_num = url_match.group(2)\n",
    "        headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',\n",
    "    }\n",
    "        with requests.get(url, headers=headers) as r: \n",
    "            logging.info(f\"{fco_type}/{id_num} status code: {r.status_code}\")\n",
    "            if r.status_code == 200: \n",
    "                s = BeautifulSoup(r.content, 'html.parser')\n",
    "                if fco_type == 'agents': \n",
    "                    self.Asoup.update({id_num:s})\n",
    "                    self.soups.update({'a'+str(id_num):s})\n",
    "                elif fco_type == 'works': \n",
    "                    self.Wsoup.update({id_num:s})\n",
    "                    self.soups.update({'w'+str(id_num):s})\n",
    "            else: \n",
    "                if fco_type == 'agents': \n",
    "                    self.aberrantAs.append({'a'+str(id_num): r.status_code})\n",
    "                elif fco_type == 'works': \n",
    "                    self.aberrantWs.append({'w'+str(id_num): r.status_code})\n",
    "                pass\n",
    "        time.sleep(.5)\n",
    "            \n",
    "    def downloadHTML(self): \n",
    "        threads = min(max_threads, len(self.urls_to_visit)) \n",
    "        print(f\"Downloading with {threads} threads\")\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor: \n",
    "            executor.map(self.get_html, self.urls_to_visit)\n",
    "\n",
    "    def parse_html(self, html): \n",
    "        htmlAttrs = list(html.body.attrs.values())[0]\n",
    "        logging.info(f\"html body attrs: {htmlAttrs}\")\n",
    "        if 'works' in list(htmlAttrs):\n",
    "            logging.info(f\"parsing work\")\n",
    "            content = html.find('div', {'class':'col-md-9 fixed-height'})\n",
    "            try: \n",
    "                author = content.div.h3.text\n",
    "            except: \n",
    "                author = \"\"\n",
    "            logging.info(f\"author: {author}\")\n",
    "            try: \n",
    "                title = content.div.h4.text\n",
    "            except: \n",
    "                title = \"\"\n",
    "            logging.info(f\"title: {title}\")\n",
    "            stanza_text = content.find_all('p',{'class':'stanza'})\n",
    "            prose_text = content.find_all('p',{'class':'text'})\n",
    "            if len(stanza_text) > len(prose_text): \n",
    "                text = stanza_text\n",
    "            elif len(stanza_text) < len(prose_text): \n",
    "                text = prose_text\n",
    "            Wtext = [x.text.replace('\\n','').strip() for x in text]\n",
    "            metaKeys = [x.text[:-1] for x in html.find('div', {'class':'card-body'}).find_all('h4')]\n",
    "            metaVals = [x.text for x in html.find('div', {'class':'card-body'}).find_all('p')]\n",
    "            metaDict = dict(zip(metaKeys, metaVals))\n",
    "            subDict = {'title': title, \n",
    "                       'text': Wtext}\n",
    "            subDict = subDict | metaDict\n",
    "            logging.info(f\"W subDict: {subDict}\")\n",
    "            self.Ws.update(subDict)\n",
    "           #return subDict\n",
    "\n",
    "        elif 'agents' in list(htmlAttrs): \n",
    "            logging.info(f\"parsing agent\")\n",
    "            dateRegEx = re.compile('[A-z]+\\s[0-9]{1,2},\\s\\d{4}')\n",
    "            Acontent = soupsDict.get(\"a1\").find('div', {'class': 'wrapper'})\n",
    "            name = Acontent.h3.text\n",
    "            #print(f\"name: {name}\")\n",
    "            subDict = {'name': name}\n",
    "            try: \n",
    "                Alife = Acontent.find('span', {'class': 'bio'}).text\n",
    "                #logging.info(f\"Alife: \\'{Alife}\\'\")\n",
    "                bday = dateRegEx.findall(Alife)[0]#.group(1)\n",
    "                logging.info(f\"bday: {bday}\")\n",
    "                dday = dateRegEx.findall(Alife)[1]#.group(2)\n",
    "                logging.info(f\"dday: {dday}\")\n",
    "                Atype = soupsDict.get(\"a1\").find('table', {'id': 'typology'}).tbody\n",
    "                #logging.info(Atype)\n",
    "                typeKeys = [x.th.text for x in Atype.findAll('tr')]\n",
    "                logging.info(f\"typeKeys: {typeKeys}\")\n",
    "                typeVals = [x.td.text for x in Atype.findAll('tr')]\n",
    "                logging.info(f\"typeVals: {typeVals}\")\n",
    "                typeDict = dict(zip(typeKeys, typeVals))\n",
    "                logging.info(f\"typeDict: {typeDict}\")\n",
    "                subDict = subDict | typeDict\n",
    "            except: \n",
    "                pass\n",
    "            logging.info(f\"A subDict: {subDict}\")\n",
    "            self.As.update(subDict)\n",
    "            #return subDict\n",
    "        \n",
    "        else: \n",
    "            logging.info(f\"Something went wrong while parsing\")\n",
    "            pass\n",
    "    \n",
    "    def parseHTML(self): \n",
    "        threads = min(max_threads, len(self.urls_to_visit)) \n",
    "        logging.info(f\"Parsing with {threads} threads\")\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=threads) as executor: \n",
    "            executor.map(self.parse_html, list(self.soups.values()))\n",
    "    \n",
    "    def run(self): \n",
    "        for t, i in list(tables.items())[:2]: \n",
    "            for j in range(1,3): \n",
    "                self.urls_to_visit.append(domain+t+str(j))\n",
    "                \n",
    "        logging.info(f\"urls: {self.urls_to_visit}\")\n",
    "        \n",
    "        logging.info(f\"Getting As and Ws\")\n",
    "        self.downloadHTML()\n",
    "        logging.info(f\"Done getting As and Ws\")\n",
    "\n",
    "        #save_obj(self.soups, 'soups')\n",
    "        #logging.info(f\"Wsoup: {self.Wsoup}\")\n",
    "        \n",
    "        logging.info(f\"Parsing As and Ws\")\n",
    "        self.parseHTML()\n",
    "        logging.info(f\"Done parsing As and Ws\")\n",
    "\n",
    "        logging.info(f\"Making dataframes\")\n",
    "        print(f\"Making AsDf\")\n",
    "        AsDf = pd.DataFrame.from_dict(self.As)\n",
    "        print(f\"Making WsDf\")\n",
    "        WsDf = pd.DataFrame.from_dict(self.Ws)        \n",
    "        logging.info(f\"Done making dataframes\")\n",
    "        \n",
    "        logging.info(f\"Writing to json\")\n",
    "        WsDf.to_json('WsDf.json')\n",
    "        AsDf.to_json('AsDf.json')\n",
    "        logging.info(f\"Done writing to json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading with 3 threads\n",
      "Making AsDf\n",
      "Making WsDf\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': \n",
    "    ParallelMAPRR().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "soupsDict = load_obj('soups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "<title>Mapping Imagined Geographies of Revolutionary Russia</title>\n",
       "<meta content=\"authenticity_token\" name=\"csrf-param\"/>\n",
       "<meta content=\"sdU+4r6B/U0Qp7cszClrvYjSemGY8NuFt7LFIardbxvDq9KVIx6x02QGUgdAk+ZeW7zpYONzuLXgxRaYq0OXJw==\" name=\"csrf-token\"/>\n",
       "<meta content=\"width=device-width, initial-scale=1, shrink-to-fit=no\" name=\"viewport\"/>\n",
       "<link href=\"https://fonts.googleapis.com\" rel=\"preconnect\"/>\n",
       "<link crossorigin=\"\" href=\"https://fonts.gstatic.com\" rel=\"preconnect\"/>\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap\" rel=\"stylesheet\"/>\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=Noto+Serif:ital,wght@0,400;0,700;1,400;1,700&amp;display=swap\" rel=\"stylesheet\"/>\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400;0,800;1,400;1,800&amp;display=swap\" rel=\"stylesheet\"/>\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=Catamaran:wght@100;900&amp;display=swap\" rel=\"stylesheet\"/>\n",
       "<link href=\"https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@900&amp;display=swap\" rel=\"stylesheet\"/>\n",
       "<!--[if lte IE 8]><script src=\"js/html5shiv.js\"></script><![endif]-->\n",
       "<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js\"></script>\n",
       "<link data-turbolinks-track=\"reload\" href=\"/assets/application-e5536c8353b18234c895600c23db95df78f360bbe371e2845bd95bcac9763af2.css\" media=\"all\" rel=\"stylesheet\">\n",
       "<script data-turbolinks-track=\"reload\" src=\"/assets/application-70ff0d299140c03c8cee48cd2efa4497df16461f286005275dae9f5f75230a48.js\"></script>\n",
       "<script>\n",
       "//<![CDATA[\n",
       "window._token = '8wMohzfJMSmxo4ehmK9DMu4ULGKzz6gHeJkHPtonTZCBfcTwqlZ9t8UCYooUFc7RPXq/Y8hMyzcv7tSH27m1rA=='\n",
       "//]]>\n",
       "</script>\n",
       "<link href=\"https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/css/select2.min.css\" rel=\"stylesheet\">\n",
       "<script src=\"https://cdn.jsdelivr.net/npm/select2@4.1.0-rc.0/dist/js/select2.min.js\"></script>\n",
       "</link></link></head>\n",
       "<body class=\"agents show\">\n",
       "<div class=\"container-fluid\" id=\"all-content\">\n",
       "<div id=\"header-content\">\n",
       "<div id=\"maprr-header-wrapper\">\n",
       "<div id=\"header-top\">\n",
       "<div id=\"logo\"><a href=\"/home\" target=\"blank\"><img alt=\"Mapping Imagined Geographies of Revolutionary Russia\" class=\"logo_image\" src=\"/images/logo.png\"/></a></div>\n",
       "</div>\n",
       "<div id=\"header-bottom\">\n",
       "<div id=\"main-menu\">\n",
       "<ul id=\"main-menu-links\">\n",
       "<li><a class=\"maprr-nav-link\" href=\"/works\" target=\"_blank\">Works</a></li>\n",
       "<li class=\"bullet-link\"><a class=\"maprr-nav-link\" href=\"/agents\" target=\"_blank\">People</a>\n",
       "<ul>\n",
       "<li class=\"first-item\"><a class=\"maprr-nav-link\" href=\"/authors\" target=\"_blank\">Authors</a></li>\n",
       "<li><a class=\"maprr-nav-link\" href=\"/publishers\" target=\"_blank\">Publishers &amp; typographers</a></li>\n",
       "<li><a class=\"maprr-nav-link\" href=\"/historical_figures\" target=\"_blank\">Historical figures</a></li>\n",
       "</ul>\n",
       "</li>\n",
       "<li class=\"bullet-link\"><a class=\"maprr-nav-link\" href=\"/locations\" target=\"_blank\">Geography</a></li>\n",
       "<li class=\"bullet-link\"><a class=\"maprr-nav-link\" href=\"/place_based_concepts\" target=\"_blank\">Place Based</a>\n",
       "<ul>\n",
       "<li class=\"first-item\"><a class=\"maprr-nav-link\" href=\"/place_based_concepts\" target=\"_blank\">Concepts</a></li>\n",
       "<li><a class=\"maprr-nav-link\" href=\"/pbc_components\" target=\"_blank\">Components</a></li>\n",
       "</ul>\n",
       "</li>\n",
       "<li class=\"bullet-link\"><a class=\"maprr-nav-link\" href=\"/multivalent_markers\" target=\"_blank\">Multivalent Markers</a></li>\n",
       "<li class=\"bullet-link\"><a class=\"maprr-nav-link\" href=\"/graphs\" target=\"_blank\">Analysis</a>\n",
       "<ul>\n",
       "<li class=\"first-item\"><a class=\"maprr-nav-link\" href=\"/graphs/analysis_one\" target=\"_blank\">Analysis One</a></li>\n",
       "<li><a class=\"maprr-nav-link\" href=\"/graphs/analysis_two\" target=\"_blank\">Analysis Two</a></li>\n",
       "<li><a class=\"maprr-nav-link\" href=\"/graphs/analysis_three\" target=\"_blank\">Analysis Three</a></li>\n",
       "</ul>\n",
       "</li>\n",
       "<li class=\"bullet-link\"><a class=\"maprr-nav-link\" href=\"#\">Resources</a>\n",
       "<ul>\n",
       "<li class=\"first-item\"><a class=\"maprr-nav-link\" href=\"#\">Students</a>\n",
       "<div class=\"main-menu-subsection\">\n",
       "<div><a class=\"maprr-sub-nav-link\" href=\"/resources/lesson_plans\" target=\"_blank\">Lesson plans</a></div>\n",
       "<div><a class=\"maprr-sub-nav-link\" href=\"#\">Language history</a></div>\n",
       "<div><a class=\"maprr-sub-nav-link\" href=\"#\">Cultural classes</a></div>\n",
       "</div>\n",
       "</li>\n",
       "<li><a class=\"maprr-nav-link\" href=\"#\">Biographies</a></li>\n",
       "<li><a class=\"maprr-nav-link\" href=\"#\">Archive</a></li>\n",
       "<li><a class=\"maprr-nav-link\" href=\"#\">Related research</a></li>\n",
       "</ul>\n",
       "</li>\n",
       "<li class=\"bullet-link\"><a class=\"maprr-nav-link\" href=\"/about\" target=\"_blank\">About</a>\n",
       "<ul>\n",
       "<li class=\"first-item\"><a class=\"maprr-nav-link\" href=\"/about\" target=\"_blank\">About</a></li>\n",
       "<li><a class=\"maprr-nav-link\" href=\"/about/faq\" target=\"_blank\">Frequently Asked Questions</a></li>\n",
       "<li><a class=\"maprr-nav-link\" href=\"/about/contributors\" target=\"_blank\">Contributors</a></li>\n",
       "</ul>\n",
       "</li>\n",
       "</ul>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<div id=\"main-content\">\n",
       "<div id=\"homepage-banner-wrapper\">\n",
       "<div id=\"homepage-banner\"><img alt=\"Mapping Imagined Geographies of Revolutionary Russia\" src=\"/images/home/banner.jpg\"/></div>\n",
       "</div>\n",
       "<div id=\"maprr-content-wrapper\">\n",
       "<div class=\"maprr-content mt-5\">\n",
       "<div class=\"maprr-subcontent\">\n",
       "<div class=\"maprr-subcontent-left content\">\n",
       "<div class=\"spacer\">\n",
       "<h4>Typology</h4>\n",
       "<table class=\"table table-sm\" id=\"typology\">\n",
       "<tbody>\n",
       "<tr>\n",
       "<th scope=\"row\">Type of agent</th>\n",
       "<td>person</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<th>Sex</th>\n",
       "<td>female</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<th>Occupations</th>\n",
       "<td>poet, critic, translator</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<th>Social strata</th>\n",
       "<td>nobility</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<th>Literary affiliations</th>\n",
       "<td>Acmeism</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<th>Political affiliations</th>\n",
       "<td>independent</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"maprr-subcontent-right content\">\n",
       "<div class=\"mb-4\">\n",
       "<h2 class=\"eng-title\">Writers</h2>\n",
       "<h2 class=\"rus-title\">Писатели</h2>\n",
       "</div>\n",
       "<div class=\"wrapper\">\n",
       "<h3>Anna Akhmatova</h3>\n",
       "<span class=\"bio\">June 23, 1889 - March 5, 1966</span>\n",
       "<h4>Names</h4>\n",
       "<li>Anna Andreevna Gorenko, <em>birth</em></li>\n",
       "<li>Anna Akhmatova, <em>alternative, preferred</em></li>\n",
       "<h4>Snac Record</h4>\n",
       "<a class=\"bio\" href=\"https://snaccooperative.org/view/26685504\" target=\"_blank\">https://snaccooperative.org/view/26685504</a>\n",
       "<h4>Associated Works</h4>\n",
       "<table class=\"table table-sm associations\">\n",
       "<tbody>\n",
       "<tr>\n",
       "<th scope=\"row\">Authored</th>\n",
       "<td>\n",
       "<li><a href=\"/works/7\" target=\"_blank\">A ty teper' tiazhelyi i unylyi…</a></li>\n",
       "<li><a href=\"/works/610\" target=\"_blank\">Angel, tri goda khranivskii menia</a></li>\n",
       "<li><a href=\"/works/621\" target=\"_blank\">Budu chernye griadki kholit', gaiduk</a></li>\n",
       "<li><a href=\"/works/17\" target=\"_blank\">Chem khuzhe etot vek…</a></li>\n",
       "<li><a href=\"/works/609\" target=\"_blank\">Chernyi son</a></li>\n",
       "<li><a href=\"/works/618\" target=\"_blank\">Dolgim vzgliadom tvoim istomlennaia</a></li>\n",
       "<li><a href=\"/works/606\" target=\"_blank\">Drugoi golos</a></li>\n",
       "<li><a href=\"/works/614\" target=\"_blank\">Epicheskie motivy</a></li>\n",
       "<li><a href=\"/works/12\" target=\"_blank\">Eto prosto, eto iasno…</a></li>\n",
       "<li><a href=\"/works/25\" target=\"_blank\">I tselyi den', svoikh pugaias' stonov…</a></li>\n",
       "<li><a href=\"/works/4\" target=\"_blank\">I v tainuiu druzhbu c vysokim…</a></li>\n",
       "<li><a href=\"/works/16\" target=\"_blank\">I vot odna ostalas' ia…</a></li>\n",
       "<li><a href=\"/works/15\" target=\"_blank\">Ia okoshka ne zavesila…</a></li>\n",
       "<li><a href=\"/works/14\" target=\"_blank\">Ia slyshu ivolgi vsegda…</a></li>\n",
       "<li><a href=\"/works/9\" target=\"_blank\">Ia sprosila u kukushki…</a></li>\n",
       "<li><a href=\"/works/6\" target=\"_blank\">Kogda o gor'koi gibeli moei…</a></li>\n",
       "<li><a href=\"/works/27\" target=\"_blank\">Kogda v toske samoubiistva…</a></li>\n",
       "<li><a href=\"/works/611\" target=\"_blank\">Lotova zhena</a></li>\n",
       "<li><a href=\"/works/615\" target=\"_blank\">MCMXXI</a></li>\n",
       "<li><a href=\"/works/622\" target=\"_blank\">Mnogim</a></li>\n",
       "<li><a href=\"/works/617\" target=\"_blank\">Na poroge belom raia</a></li>\n",
       "<li><a href=\"/works/616\" target=\"_blank\">Ne byvat' tebe v zhivykh</a></li>\n",
       "<li><a href=\"/works/608\" target=\"_blank\">Ne s temi ia, kto brosil zemliu</a></li>\n",
       "<li><a href=\"/works/22\" target=\"_blank\">Noch'iu</a></li>\n",
       "<li><a href=\"/works/13\" target=\"_blank\">O net, ia ne tebia liubila…</a></li>\n",
       "<li><a href=\"/works/24\" target=\"_blank\">Pesenka</a></li>\n",
       "<li><a href=\"/works/604\" target=\"_blank\">Petrograd, 1919</a></li>\n",
       "<li><a href=\"/works/8\" target=\"_blank\">Plennik chuzhoi! Mne chuzhogo ne nado…</a></li>\n",
       "<li><a href=\"/works/10\" target=\"_blank\">Po nedele ni slova ni s kem ne skazhu…</a></li>\n",
       "<li><a href=\"/works/19\" target=\"_blank\">Po tverdomu grebniu sugroba…</a></li>\n",
       "<li><a href=\"/works/28\" target=\"_blank\">Pokinuv roshchi rodiny sviashchennoi…</a></li>\n",
       "<li><a href=\"/works/612\" target=\"_blank\">Prichitanie</a></li>\n",
       "<li><a href=\"/works/3\" target=\"_blank\">Prosypat'sia na rassvete…</a></li>\n",
       "<li><a href=\"/works/619\" target=\"_blank\">Shiroko paspakhnuty vorota</a></li>\n",
       "<li><a href=\"/works/607\" target=\"_blank\">Skazal, chto u menia sopernits net</a></li>\n",
       "<li><a href=\"/works/5\" target=\"_blank\">Slovno angel, vozmutivshii vodu…</a></li>\n",
       "<li><a href=\"/works/29\" target=\"_blank\">Smerkaetsia, i v nebe temno-sinem…</a></li>\n",
       "<li><a href=\"/works/1\" target=\"_blank\">Srazu stalo tikho v dome…</a></li>\n",
       "<li><a href=\"/works/23\" target=\"_blank\">Techet reka nespeshno…</a></li>\n",
       "<li><a href=\"/works/18\" target=\"_blank\">Teper' nikto ne stanet slushat' pesen…</a></li>\n",
       "<li><a href=\"/works/20\" target=\"_blank\">Teper' proshchai stolitsa…</a></li>\n",
       "<li><a href=\"/works/620\" target=\"_blank\">Tot avgust, kak zheltoe plamia</a></li>\n",
       "<li><a href=\"/works/2\" target=\"_blank\">Ty - otstupnik: za ostrov zelenyi…</a></li>\n",
       "<li><a href=\"/works/26\" target=\"_blank\">Ty mog by mne snit'sia i rezhe...</a></li>\n",
       "<li><a href=\"/works/613\" target=\"_blank\">Vot i bereg severnogo moria</a></li>\n",
       "<li><a href=\"/works/30\" target=\"_blank\">Zare</a></li>\n",
       "<li><a href=\"/works/11\" target=\"_blank\">Zemnaia slava kak dym…</a></li>\n",
       "<li><a href=\"/works/21\" target=\"_blank\">Zhdala ego naprasno mnogo let…</a></li>\n",
       "<li><a href=\"/works/605\" target=\"_blank\">БЕЖЕЦК</a></li>\n",
       "</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "<h4>Associated Locations</h4>\n",
       "<table class=\"table table-sm associations\">\n",
       "<tbody>\n",
       "<tr>\n",
       "<th scope=\"row\">Birthplace</th>\n",
       "<td><li><a href=\"/locations/106\" target=\"_blank\">Odessa</a></li></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<th scope=\"row\">Deathplace</th>\n",
       "<td><li><a href=\"/locations/38\" target=\"_blank\">Moscow</a></li></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<th scope=\"row\">Corporate Address(es)</th>\n",
       "<td>\n",
       "<li><a href=\"/locations/139\" target=\"_blank\">Slepnevo</a></li>\n",
       "<li><a href=\"/locations/140\" target=\"_blank\">Tsarskoe selo</a></li>\n",
       "</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "<div class=\"narrow-dot-spacer\"><span>· · · · · ·</span></div>\n",
       "<div data-agentid=\"1\" id=\"barchart\">\n",
       "<h3>Visualizations</h3>\n",
       "<div><a class=\"graph-new-window\" data-divid=\"open-instances-pbc\" href=\"#\">View a Graph Showing Writer's Place Based Concepts by Instance Count</a>\n",
       "<div id=\"open-instances-pbc\" style=\"display:none\">\n",
       "<span class=\"ml-2\">Place based concepts by instance count</span>\n",
       "<div class=\"chart-container\" id=\"instances-pbc\"></div>\n",
       "</div>\n",
       "</div>\n",
       "<div><a class=\"graph-new-window\" data-divid=\"open-works-pbc\" href=\"#\">View a Graph Showing Writer's Place Based Concepts by Work Count</a>\n",
       "<div id=\"open-works-pbc\" style=\"display:none\">\n",
       "<span class=\"ml-2\">Place based concepts by work count</span>\n",
       "<div class=\"chart-container\" id=\"works-pbc\"></div>\n",
       "</div>\n",
       "</div>\n",
       "<div><a class=\"graph-new-window\" data-divid=\"open-instances-mm\" href=\"#\">View a Graph Showing Writer's Multivalent Markers by Instance Count</a>\n",
       "<div id=\"open-instances-mm\" style=\"display:none\">\n",
       "<span class=\"ml-2\">Multivalent Markers by instance count</span>\n",
       "<div class=\"chart-container\" id=\"instances-mm\"></div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div><!-- right content -->\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<div id=\"footer-content\">\n",
       "<div id=\"maprr-footer-wrapper\">\n",
       "<div id=\"footer\">\n",
       "<div id=\"footer-top-wrapper-wrapper\">\n",
       "<div id=\"footer-top-wrapper\">\n",
       "<div id=\"footer-top\">\n",
       "<div id=\"footer-top-left\">\n",
       "<div id=\"footer-uva-iath\">\n",
       "<a href=\"https://www.virginia.edu\" target=\"_blank\"><img alt=\"UVA\" id=\"footer-uva\" src=\"/images/footer-uva.png\"/></a>\n",
       "<span id=\"footer-uva-iath-line\"> </span>\n",
       "<a href=\"http://www.iath.virginia.edu\" target=\"_blank\"><img alt=\"IATH\" id=\"footer-iath\" src=\"/images/footer-iath.png\"/></a>\n",
       "</div>\n",
       "</div>\n",
       "<div id=\"footer-top-middle\">\n",
       "<div id=\"footer-contact\"><a href=\"http://maprr.iath.virginia.edu/contact.php\" target=\"_blank\">Contact us &gt;</a></div>\n",
       "</div>\n",
       "<div id=\"footer-top-right\">\n",
       "<div id=\"footer-last_modified\">\n",
       "<div>Last Modified: May 3, 2022 18:47 GMT</div>\n",
       "<div>© 2019-2022 by the University of Virginia</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<div id=\"footer-bottom-wrapper-wrapper\">\n",
       "<div id=\"footer-bottom-wrapper\">\n",
       "<div id=\"footer-bottom\">\n",
       "<div id=\"footer-bottom-left\">\n",
       "<div class=\"footer-bottom-content\">\n",
       "<div class=\"footer-bottom-header\">The Institute for Advanced Technology in the Humanities</div>\n",
       "<div class=\"footer-bottom-text\">University of Virginia\n",
       "                <br/>P.O. Box 400115 Charlottesville, VA 22904-4115\n",
       "                <br/><br/><span class=\"small_caps\">t</span> 434.924.4527 &amp;vert <span class=\"small_caps\">f</span> 434.982.2363\n",
       "                </div>\n",
       "</div>\n",
       "</div>\n",
       "<div id=\"footer-bottom-middle\">\n",
       "<div class=\"footer-bottom-content\">\n",
       "<div class=\"footer-bottom-header\">Sponsors</div>\n",
       "<div class=\"footer-sponsor\">\n",
       "<div class=\"footer-sponsor-header\"><a href=\"http://www.iath.virginia.edu/\" target=\"_blank\">IATH</a></div>\n",
       "<div class=\"footer-sponsor-logo\"><a href=\"http://www.iath.virginia.edu/\" target=\"_blank\"><img alt=\"IATH\" src=\"/images/sponsors/iath.png\"/></a></div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<div id=\"footer-bottom-right\">\n",
       "<div class=\"footer-bottom-header\"> </div>\n",
       "<div class=\"footer-bottom-content\">\n",
       "<div class=\"footer-sponsor\">\n",
       "<div class=\"footer-sponsor-header\"><a href=\"https://provost.virginia.edu/subsite/academic-outreach\">UVA Vice Provost for Academic Outreach</a></div>\n",
       "<div class=\"footer-sponsor-logo\"><a href=\"https://provost.virginia.edu/subsite/academic-outreach\"><img alt=\"UVA Vice Provost for Academic Outreach\" src=\"/images/sponsors/university_of_virginia.png\"/></a></div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<div id=\"ajax_request_loading\">\n",
       "<img class=\"img-responsive\" src=\"/images/ajax-loader.gif\"/>\n",
       "</div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soupsDict.get(\"a1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Anna Akhmatova\n",
      "bday: June 23, 1889\n",
      "dday: March 5, 1966\n",
      "typeDict: {'Type of agent': 'person', 'Sex': 'female', 'Occupations': 'poet, critic, translator', 'Social strata': 'nobility', 'Literary affiliations': 'Acmeism', 'Political affiliations': 'independent'}\n"
     ]
    }
   ],
   "source": [
    "dateRegEx = re.compile('[A-z]+\\s[0-9]{1,2},\\s\\d{4}')\n",
    "Acontent = soupsDict.get(\"a1\").find('div', {'class': 'wrapper'})\n",
    "name = Acontent.h3.text\n",
    "print(f\"name: {name}\")\n",
    "Alife = Acontent.find('span', {'class': 'bio'}).text\n",
    "#print(f\"Alife: \\'{Alife}\\'\")\n",
    "bday = dateRegEx.findall(Alife)[0]#.group(1)\n",
    "print(f\"bday: {bday}\")\n",
    "dday = dateRegEx.findall(Alife)[1]#.group(2)\n",
    "print(f\"dday: {dday}\")\n",
    "Atype = soupsDict.get(\"a1\").find('table', {'id': 'typology'}).tbody\n",
    "#print(Atype)\n",
    "typeKeys = [x.th.text for x in Atype.findAll('tr')]\n",
    "#print(f\"typeKeys: {typeKeys}\")\n",
    "typeVals = [x.td.text for x in Atype.findAll('tr')]\n",
    "#print(f\"typeVals: {typeVals}\")\n",
    "typeDict = dict(zip(typeKeys, typeVals))\n",
    "print(f\"typeDict: {typeDict}\"t)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\" = AsDf.append(maprr().get_single(cat='agent', id_num=45))\n",
    "\n",
    "AsDf['birth'] = pd.to_datetime(AsDf['birth'], errors='coerce', infer_datetime_format=True)\n",
    "AsDf['death'] = pd.to_datetime(AsDf['death'], errors='coerce', infer_datetime_format=True)\n",
    "AsDf.sort_index()#[AsDf.name.str.contains('Osip')]\n",
    "AsDf.to_json('AsDf.json')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "AsDf = pd.read_json('AsDf.json')\n",
    "AsDf['birth'] = pd.to_datetime(AsDf['birth'], errors='coerce', infer_datetime_format=True)\n",
    "AsDf['death'] = pd.to_datetime(AsDf['death'], errors='coerce', infer_datetime_format=True)\n",
    "AsDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### libDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "libDf = pd.read_json('WsDf.json')\n",
    "libDf.index.name = 'w_id'\n",
    "libDf.columns = lib_cols\n",
    "libDf['comp_date'] = pd.to_datetime(libDf['comp_date'], errors='coerce')\n",
    "libDf['pub_year'] = pd.to_datetime(libDf['pub_year'], errors='coerce')\n",
    "libDf['pub_year'] = libDf.pub_year.apply(lambda x: x.year).astype('int64', errors='ignore')\n",
    "print(libDf.shape)\n",
    "#type(libDf.loc[1, 'pub_year'])\n",
    "libDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### authorsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "authorsDf = libDf.reset_index().groupby('author').size().to_frame().rename(columns={0:'num_works'})\n",
    "authorsDf['num_words'] = libDf.reset_index().groupby('author').sum().num_words\n",
    "authorsDf['avg_wpw'] = round(authorsDf.num_words/authorsDf.num_works, 2)\n",
    "authorsDf = authorsDf.reset_index().sort_values(by=['avg_wpw'], ascending=False).rename(columns={'author':'name'})\n",
    "authorsDf = pd.merge(AsDf.reset_index(), authorsDf.reset_index(), how='right', on='name').set_index('index_x')\n",
    "authorsDf.index.name = 'a_id'\n",
    "authorsDf.to_json('authorsDf.json', date_format='iso')\n",
    "authorsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authorsDf = pd.read_json('authorsDf.json')\n",
    "#authorsDf[['birth', 'death']] = authorsDf[['birth', 'death']].apply(pd.to_datetime, format=\"%Y-%m-%d\")\n",
    "#[dt.to_datetime().date() for dt in authorsDf[['birth', 'death']]]\n",
    "authorsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(authorsDf.loc[15, 'death'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### worksDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textsDf = libDf[['text']]\n",
    "textsDf['num_words'] = libDf['num_words'] = textsDf.text.apply(lambda k: len([a for b in [x.split() for y in k for x in y.split('               ')] for a in b if a.isalpha() == True]))\n",
    "textsDf.sort_values('num_words', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worksDf = libDf[['title','year','author','genre','num_lps','num_words']]\n",
    "worksDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpDf = libDf[['text']]\n",
    "lpDf = lpDf.text.apply(lambda x: pd.Series([y for y in x])).stack().to_frame().rename(columns={0:'lp_str'})\n",
    "lpDf.index.names = OHCO[:2]\n",
    "lpDf\n",
    "tokenDf = lpDf.lp_str.apply(lambda x: tokenize(x)).to_frame()#.rename(columns={0:'token'})\n",
    "#tokenDf = lpDf.lp_str.apply(lambda x: y.text for y in tokenize(x)[1])\n",
    "tokenDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tokenize(lpDf.lp_str): \n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "293d930ecca589385d8136ae0476c1fde7dc48a83b398052354be2f05bed9ba3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
