{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAPRR Textual Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import pandas as pd \n",
    "import re\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "from natasha import (\n",
    "    Segmenter, \n",
    "    MorphVocab, \n",
    "    NewsEmbedding, \n",
    "    NewsMorphTagger, \n",
    "    NewsSyntaxParser, \n",
    "    NewsNERTagger, \n",
    "    PER, \n",
    "    NamesExtractor, \n",
    "    Doc)\n",
    "from razdel import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='maprr_out.log', encoding='utf-8', format='%(asctime)s %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_cols = ['title_ru', 'genre', 'text', 'title_en', '1st_line', 'author', 'comp_date', 'comp_loc', 'pub_src', '1st_pub', 'pub_year', 'pub_loc']\n",
    "a_cols = ['name', 'birth', 'death', 'a_type', 'sex', 'occs', 'fam_soc_str', 'lit_affil', 'pol_affil', 'corp_type', 'corp_affil']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'https://mpgrr.herokuapp.com/'\n",
    "tables = {\n",
    "    'agents/': 304, \n",
    "    'works/': 603, \n",
    "    'place_based_concepts/': 315, \n",
    "    'locations/': 366, \n",
    "    'multivalent_markers/': 433\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "allURLs = [domain+item[0]+str(i) for item in tables.items() for i in range(1, item[1]+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls_to_visit = [] \n",
    "for t, i in list(tables.items())[:2]: \n",
    "    for j in range(0,i+1): \n",
    "        urls_to_visit.append(domain+t+str(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class maprr: \n",
    "    \n",
    "    def __init__(self): \n",
    "        self.Wsoup = {} \n",
    "        self.Asoup = {}\n",
    "        self.Ws = {}\n",
    "        self.As = {}\n",
    "    \n",
    "    def get_htmlA(self): \n",
    "        \"\"\"This function uses the list of Agent IDs from the tables dict above\n",
    "        and the grabs it using requests before putting the html reponse in Asoup\"\"\"\n",
    "        \n",
    "        # initialize list of pages that don't return 200\n",
    "        aberrantAs = []\n",
    "        # go through list of Agents from 1 to the number defined in tables\n",
    "        for i in range(1, (list(tables.values())[0]+1)):\n",
    "        #for i in range(1, 11):\n",
    "            # make url\n",
    "            url = domain+list(tables.keys())[0]+str(i) \n",
    "            # initialize connection to url \n",
    "            with requests.get(url) as r: \n",
    "                # log status code \n",
    "                logging.info(f\"A{i} status code: {r.status_code}\")\n",
    "                # if connection is successful\n",
    "                if r.status_code == 200: \n",
    "                    # make soup from html\n",
    "                    s = BeautifulSoup(r.content, 'html.parser') \n",
    "                    # add to list of A soups\n",
    "                    self.Asoup.update({i:s})\n",
    "                else: \n",
    "                    # if connection is not successful, add to list\n",
    "                    aberrantAs.append((i, r.status_code))\n",
    "                    pass\n",
    "                # wait a hot second or the server gets >:( \n",
    "                time.sleep(.1)\n",
    "        # report list of A errors if there is one (there always is)\n",
    "        if len(aberrantAs) > 0: \n",
    "            print(f\"Aberrant agent pages are #s {aberrantAs}\")\n",
    "        else: \n",
    "            print(f\"There were no aberrant agent pages!\")\n",
    "\n",
    "    def get_htmlW(self): \n",
    "        \"\"\"This function uses the list of Work IDs from the tables dict above\n",
    "        and the grabs it using requests before putting the html reponse in Wsoup\"\"\"\n",
    "        \n",
    "        # initialize list of pages that don't return 200\n",
    "        aberrantWs = []\n",
    "        # go through list of Words from 1 to the number defined in tables\n",
    "        for i in range(1, (list(tables.values())[1]+1)):\n",
    "        #for i in range(1, 11):\n",
    "            # make url\n",
    "            url = domain+list(tables.keys())[1]+str(i)\n",
    "            # initialize connection to url \n",
    "            with requests.get(url) as r: \n",
    "                # log status code\n",
    "                logging.info(f\"W{i} status code: {r.status_code}\")\n",
    "                # if connection is successful\n",
    "                if r.status_code == 200: \n",
    "                    # make soup from html\n",
    "                    s = BeautifulSoup(r.content, 'html.parser')\n",
    "                    # add to list of A soups\n",
    "                    self.Wsoup.update({i:s})\n",
    "                else: \n",
    "                    # if connection is not successful, add to list\n",
    "                    aberrantWs.append((i, r.status_code))\n",
    "                    pass\n",
    "                # wait a hot second or the server gets >:( \n",
    "                time.sleep(.1)\n",
    "        # report list of A errors if there is one (there always is)\n",
    "        if len(aberrantWs) > 0: \n",
    "            print(f\"Aberrant work pages are #s {aberrantWs}\")\n",
    "        else: \n",
    "            print(f\"There were no aberrant work pages!\")\n",
    "\n",
    "    def parseWs(self, html): \n",
    "        \"\"\"This function retrieves Work info from the HTML provided\"\"\"\n",
    "        \n",
    "        # extract things we will need\n",
    "        content = html.find('div', {'class':'col-md-9 fixed-height'})\n",
    "        # try to locate author text of Work\n",
    "        try: \n",
    "            author = content.div.h3.text\n",
    "        except: \n",
    "            author = \"unknown\"\n",
    "        # try to locate title text of Work\n",
    "        try: \n",
    "            title = content.div.h4.text\n",
    "        except: \n",
    "            title = \"untitled\"\n",
    "        # try for both stanza and para text because they return [] \n",
    "        stanza_text = content.find_all('p',{'class':'stanza'})\n",
    "        prose_text = content.find_all('p',{'class':'text'})\n",
    "        # decide which text to use based on length \n",
    "        if len(prose_text) > len(stanza_text): \n",
    "            text = prose_text\n",
    "            genre = 'prose'\n",
    "        else: \n",
    "            text = stanza_text\n",
    "            genre = 'poetry'\n",
    "        # get actual text \n",
    "        Wtext = [x.text.replace('\\n','').strip() for x in text]\n",
    "        # make list of keys of Work types\n",
    "        typeKeys = [x.text[:-1] for x in html.find('div', {'class':'card-body'}).find_all('h4')]\n",
    "        # make list of values of Work types \n",
    "        typeVals = [x.text for x in html.find('div', {'class':'card-body'}).find_all('p')]\n",
    "        # make dictionary of keys:values from above\n",
    "        typeDict = dict(zip(typeKeys, typeVals))\n",
    "        # initialize sub dictionary of Work\n",
    "        Wdict = {'title': title, \n",
    "                   'genre': genre,\n",
    "                   'text': Wtext} \n",
    "        # add type keys and values to complete sub dictionary of Work\n",
    "        Wdict.update(typeDict) \n",
    "        # return Work dict to be made into a DataFrame row\n",
    "        return Wdict\n",
    "\n",
    "    def parseAs(self, html): \n",
    "        \"\"\"This function retrieves Agent info from the HTML provided\"\"\"\n",
    "        \n",
    "        # get Agent's name\n",
    "        name = html.find('div', {'class': 'card scrollable'}).h2.text \n",
    "        # get Agent's birth- and deathdates \n",
    "        bdate, ddate = html.find('div', {'class': 'card scrollable'}).span.text.split(' - ') \n",
    "        # initialize dictionary of Agent\n",
    "        Adict = {'name': name, 'birth': bdate, 'death': ddate}\n",
    "        # make list of type keys\n",
    "        typeKeys = [x.h4.text.lower().replace(' ','_') for x in html.find_all('div', {'class': 'col-md-4'})]\n",
    "        # initialize list of type values\n",
    "        typeVals = []\n",
    "        # add type values to list if found or default to 'unknown' (though some real values are also 'unknown')\n",
    "        for typ in html.find_all('div', {'class': 'col-md-4'}): \n",
    "            try: \n",
    "                typeVals.append(typ.p or typ.div.span.text)\n",
    "            except: \n",
    "                typeVals.append(\"unknown\")\n",
    "        # for some reason keys are at different levels and require '.text.' attribute but of course some don't\n",
    "        typeVals = [x.text if not isinstance(x, str) else x for x in typeVals]\n",
    "        # make dictionary of keys:values from above\n",
    "        typeDict = dict(zip(typeKeys, typeVals))\n",
    "        # add type keys and values to complete sub dictionary of Work\n",
    "        Adict.update(typeDict) \n",
    "        # return Agent dict to be made into a DataFrame row \n",
    "        return Adict\n",
    "    \n",
    "    def get_single(self, cat, id_num): \n",
    "        \"\"\"This function combines the functions above and returns the DataFrame row\"\"\" \n",
    "        \n",
    "        # make url from parameters and initialize request \n",
    "        with requests.get(domain+cat+'s/'+str(id_num)) as r: \n",
    "            # double check the URL is correct\n",
    "            print(r.url)\n",
    "            # check status code\n",
    "            print(f\"{cat+str(id_num)} status code: {r.status_code}\")\n",
    "            # if connection is successful\n",
    "            if r.status_code == 200: \n",
    "                # make soup from html content \n",
    "                s = BeautifulSoup(r.content, 'html.parser')\n",
    "                # sort by FOO type \n",
    "                if cat.lower() == 'work': \n",
    "                    # make dictionary if Work with parseWs function\n",
    "                    SubDict = {id_num:self.parseWs(s)}\n",
    "                elif cat.lower() == 'agent': \n",
    "                    # make dictionary if Agent with parseAs function\n",
    "                    SubDict = {id_num:self.parseAs(s)} \n",
    "                else: \n",
    "                    print(\"You need a category: 'work' or 'agent'...\") \n",
    "                # make DataFrame row from dictionary\n",
    "                singleDf = pd.DataFrame.from_dict(newSubDict, orient='index')\n",
    "                # return DataFrame row\n",
    "                return singleDf\n",
    "            # return status code if connection is unsuccessful\n",
    "            else: \n",
    "                print(f\"Error: {r.status_code}\")\n",
    "\n",
    "\n",
    "    def run(self): \n",
    "        \"\"\"This function runs retrieval and parsing using the functions above, creates DataFrames, and persists them (JSON)\"\"\"\n",
    "        \n",
    "        logging.info(f\"Getting As and Ws\")\n",
    "        print(f\"Getting As\")\n",
    "        # get starting time for A retrieval\n",
    "        at1 = time.time()\n",
    "        # retrieve As\n",
    "        self.get_htmlA() \n",
    "        # get finishing time for A retrieval\n",
    "        at2 = time.time()\n",
    "        # display run times for A retrieval \n",
    "        print(f\"Got As in {round((at2-at1), 3)} sec ({round(((at2-at1)/list(tables.values())[0]), 5)} sec/ea.)\")\n",
    "        logging.info(f\"Got As in {round((at2-at1), 3)} sec ({round(((at2-at1)/list(tables.values())[0]), 5)} sec/ea.)\")\n",
    "        \n",
    "        print(f\"Getting Ws\")\n",
    "        # get starting time for W retrieval\n",
    "        wt1 = time.time()\n",
    "        # retrieve As\n",
    "        self.get_htmlW() \n",
    "        # get finishing time for W retrieval\n",
    "        wt2 = time.time()\n",
    "        # display run times for W retrieval \n",
    "        print(f\"Got Ws in {round((wt2-wt1), 3)} sec ({round(((wt2-wt1)/list(tables.values())[1]), 5)} sec/ea.)\")\n",
    "        logging.info(f\"Got Ws in {round((wt2-wt1), 3)} sec ({round(((wt2-wt1)/list(tables.values())[1]), 5)} sec/ea.)\")\n",
    "        logging.info(f\"Done getting As and Ws\")\n",
    "        \n",
    "        logging.info(f\"Parsing As and Ws\")\n",
    "        print(f\"Parsing As\")\n",
    "        # get starting time for A parsing\n",
    "        pa1 = time.time()\n",
    "        # parse Agent HTML instances\n",
    "        self.As = {k: self.parseAs(v) for k, v in self.Asoup.items()}\n",
    "        # get finishing time for A parsing\n",
    "        pa2 = time.time()\n",
    "        # display run times for A parsing \n",
    "        print(f\"Parsed As in {round((pa2-pa1), 3)} sec ({round((pa2-pa1)/len(list(self.Asoup.items())), 5)} sec/ea.)\")\n",
    "        logging.info(f\"Parsed As in {round((pa2-pa1), 3)} sec ({round((pa2-pa1)/len(list(self.Asoup.items())), 5)} sec/ea.)\")\n",
    "        \n",
    "        print(f\"Parsing Ws\")\n",
    "        # get starting time for W parsing\n",
    "        pw1 = time.time()\n",
    "        # parse Work HTML instances \n",
    "        self.Ws = {k: self.parseWs(v) for k, v in self.Wsoup.items()}\n",
    "        # get finishing time for W parsing\n",
    "        pw2 = time.time() \n",
    "        # display run times for W parsing \n",
    "        print(f\"Parsed Ws in {round((pw2-pw1), 3)} sec ({round((pw2-pw1)/len(list(self.Wsoup.items())), 5)} sec/ea.)\")\n",
    "        logging.info(f\"Parsed Ws in {round((pw2-pw1), 3)} sec ({round((pw2-pw1)/len(list(self.Wsoup.items())), 5)} sec/ea.)\")\n",
    "        logging.info(f\"Done parsing As and Ws\")\n",
    "        \n",
    "        logging.info(f\"Making dataframes\")\n",
    "        print(f\"Making AsDf\")\n",
    "        # create DataFrame of Agent dictionaries\n",
    "        AsDf = pd.DataFrame.from_dict(self.As, orient='index')\n",
    "        print(f\"Making WsDf\")\n",
    "        # create DataFrame of Work dictionaries\n",
    "        WsDf = pd.DataFrame.from_dict(self.Ws, orient='index')  \n",
    "        logging.info(f\"Done making dataframes\")\n",
    "        \n",
    "        logging.info(f\"Writing to json\")\n",
    "        # write Work DataFrame to JSON\n",
    "        WsDf.to_json('WsDf.json')\n",
    "        # write Agent DataFrame to JSON\n",
    "        AsDf.to_json('AsDf.json')\n",
    "        logging.info(f\"Done writing to json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_status(urls): \n",
    "    aberrantURLs = []\n",
    "    logging.info(f\"Checking status of URLs\")\n",
    "    for url in urls: \n",
    "        logging.info(f\"Trying {url}\")\n",
    "        with requests.get(url) as r: \n",
    "            if r.status_code == 200: \n",
    "                #logging.info(f\"{url} successful\")\n",
    "                pass\n",
    "            else: \n",
    "                logging.info(f\"{url}: {r.status_code}\")\n",
    "                aberrantURLs.append({url: r.status_code})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting As\n",
      "Aberrant agent pages are #s [(74, 404), (75, 404), (76, 404), (77, 404), (139, 404), (140, 404), (192, 404), (206, 404), (252, 404)]\n",
      "Got As in 216.721 sec (0.7129 sec/ea.)\n",
      "Getting Ws\n",
      "Aberrant work pages are #s [(173, 500), (179, 500), (261, 500), (306, 500), (425, 500), (433, 500), (434, 500), (435, 500), (439, 500), (526, 500), (554, 500), (576, 500), (577, 500), (578, 500), (579, 500), (581, 500), (598, 500)]\n",
      "Got Ws in 525.414 sec (0.87133 sec/ea.)\n",
      "Parsing As\n",
      "Parsed As in 0.393 sec (0.00133 sec/ea.)\n",
      "Parsing Ws\n",
      "Parsed Ws in 0.628 sec (0.00107 sec/ea.)\n",
      "Making AsDf\n",
      "Making WsDf\n",
      "CPU times: user 1min 22s, sys: 1.64 s, total: 1min 24s\n",
      "Wall time: 12min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ == '__main__': \n",
    "    maprr().run()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "class MAPRR: \n",
    "    \n",
    "    def __init__(self): \n",
    "        self.urls_to_visit = []\n",
    "        self.aberrantAs = []\n",
    "        self.aberrantWs = []\n",
    "        self.soups = {}\n",
    "        self.Wsoup = {} \n",
    "        self.Asoup = {}\n",
    "        self.Ws = {}\n",
    "        self.As = {}\n",
    "    \n",
    "    def get_html(self, url): \n",
    "        url_format = 'https://mpgrr.herokuapp.com/(\\w+)/(\\d{1,3})'\n",
    "        url_match = re.match(url_format, url)\n",
    "        fco_type = url_match.group(1)\n",
    "        id_num = url_match.group(2)\n",
    "        headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',\n",
    "    }\n",
    "        with requests.get(url, headers=headers) as r: \n",
    "            logging.info(f\"{fco_type}/{id_num} status code: {r.status_code}\")\n",
    "            if r.status_code == 200: \n",
    "                s = BeautifulSoup(r.content, 'html.parser')\n",
    "                #self.soups.update({id_num:s})\n",
    "                if fco_type == 'agents': \n",
    "                    self.Asoup.update({id_num:s})\n",
    "                elif fco_type == 'works': \n",
    "                    self.Wsoup.update({id_num:s})\n",
    "            else: \n",
    "                if fco_type == 'agents': \n",
    "                    self.aberrantAs.append({'A'+str(id_num): r.status_code})\n",
    "                elif fco_type == 'works': \n",
    "                    self.aberrantWs.append({'W'+str(id_num): r.status_code})\n",
    "                pass\n",
    "        time.sleep(.5)\n",
    "        \n",
    "    def parse_html(self, html): \n",
    "        if 'works' in list(html.body.attrs.values())[0]: \n",
    "            content = html.find('div', {'class':'col-md-9 fixed-height'})\n",
    "            try: \n",
    "                author = content.div.h3.text\n",
    "            except: \n",
    "                author = \"\"\n",
    "            try: \n",
    "                title = content.div.h4.text\n",
    "            except: \n",
    "                title = \"\"\n",
    "            stanza_text = content.find_all('p',{'class':'stanza'})\n",
    "            prose_text = content.find_all('p',{'class':'text'})\n",
    "            if len(stanza_text) > len(prose_text): \n",
    "                text = stanza_text\n",
    "            elif len(stanza_text) < len(prose_text): \n",
    "                text = prose_text\n",
    "            Wtext = [x.text.replace('\\n','').strip() for x in text]\n",
    "            metaKeys = [x.text[:-1] for x in html.find('div', {'class':'card-body'}).find_all('h4')]\n",
    "            metaVals = [x.text for x in html.find('div', {'class':'card-body'}).find_all('p')]\n",
    "            metaDict = dict(zip(metaKeys, metaVals))\n",
    "            subDict = {'title': title, \n",
    "                       'text': Wtext}\n",
    "\n",
    "            #self.Ws.update(subdict)\n",
    "            return subDict\n",
    "\n",
    "        elif 'agents' in list(html.body.attrs.values())[0]: \n",
    "            name = html.find('div', {'class': 'card scrollable'}).h2.text\n",
    "            bdate, ddate = html.find('div', {'class': 'card scrollable'}).span.text.split(' - ')\n",
    "            subDict = {'name': name, 'birth': bdate, 'death': ddate}\n",
    "            try: \n",
    "                typeKeys = [x.h4.text for x in html.find_all('div', {'class': 'col-md-4'})]\n",
    "                typeVals = [x.p or x.div.span.text for x in html.find_all('div', {'class': 'col-md-4'})]\n",
    "                typeVals[:2] = [x.text for x in typeVals[:2]]\n",
    "                typeDict = dict(zip(typeKeys, typeVals))\n",
    "                subDict.update(typeDict)\n",
    "            except: \n",
    "                pass\n",
    "\n",
    "            #self.As.update(subdict)\n",
    "            return subDict\n",
    "\n",
    "    def run(self): \n",
    "        for t, i in list(tables.items())[:2]: \n",
    "            for j in range(0,5): \n",
    "                self.urls_to_visit.append(domain+t+str(j))\n",
    "        \n",
    "        logging.info(f\"Getting As and Ws\")\n",
    "        for url in self.urls_to_visit: \n",
    "            self.get_html(url) \n",
    "        logging.info(f\"Done getting As and Ws\")\n",
    "        \n",
    "        logging.info(f\"Parsing As and Ws\")\n",
    "        print(f\"Parsing As\")\n",
    "        self.As = {k: self.parse_html(v) for k, v in self.Asoup.items()}\n",
    "        print(f\"Parsing Ws\")\n",
    "        self.Ws = {k: self.parse_html(v) for k, v in self.Wsoup.items()}        \n",
    "        logging.info(f\"Done parsing As and Ws\")\n",
    "        \n",
    "        logging.info(f\"Making dataframes\")\n",
    "        print(f\"Making AsDf\")\n",
    "        AsDf = pd.DataFrame.from_dict(self.As, orient='index')\n",
    "        print(f\"Making WsDf\")\n",
    "        WsDf = pd.DataFrame.from_dict(self.Ws, orient='index')        \n",
    "        logging.info(f\"Done making dataframes\")\n",
    "        \n",
    "        logging.info(f\"Writing to json\")\n",
    "        WsDf.to_json('WsDf.json')\n",
    "        AsDf.to_json('AsDf.json')\n",
    "        logging.info(f\"Done writing to json\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "class ParallelMAPRR: \n",
    "    \n",
    "    global domain, tables, max_threads\n",
    "    \n",
    "    def __init__(self): \n",
    "        self.urls_to_visit = []\n",
    "        self.aberrantAs = []\n",
    "        self.aberrantWs = []\n",
    "        self.soups = {}\n",
    "        self.Wsoup = {} \n",
    "        self.Asoup = {}\n",
    "        self.Ws = {}\n",
    "        self.As = {}\n",
    "    \n",
    "    def get_html(self, url): \n",
    "        url_format = 'https://mpgrr.herokuapp.com/(\\w+)/(\\d{1,3})'\n",
    "        url_match = re.match(url_format, url)\n",
    "        fco_type = url_match.group(1)\n",
    "        id_num = url_match.group(2)\n",
    "        headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',\n",
    "    }\n",
    "        with requests.get(url, headers=headers) as r: \n",
    "            logging.info(f\"{fco_type}/{id_num} status code: {r.status_code}\")\n",
    "            if r.status_code == 200: \n",
    "                s = BeautifulSoup(r.content, 'html.parser')\n",
    "                self.soups.update({id_num:s})\n",
    "                #if fco_type == 'agents': \n",
    "                #    self.Asoup.update({id_num:s})\n",
    "                #elif fco_type == 'works': \n",
    "                #    self.Wsoup.update({id_num:s})\n",
    "            else: \n",
    "                if fco_type == 'agents': \n",
    "                    self.aberrantAs.append({'A'+str(id_num): r.status_code})\n",
    "                elif fco_type == 'works': \n",
    "                    self.aberrantWs.append({'W'+str(id_num): r.status_code})\n",
    "                pass\n",
    "        time.sleep(.5)\n",
    "            \n",
    "    def downloadHTML(self): \n",
    "        threads = min(max_threads, len(self.urls_to_visit)) \n",
    "        print(f\"Downloading with {threads} threads\")\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor: \n",
    "            executor.map(self.get_html, self.urls_to_visit)\n",
    "\n",
    "    def parse_html(self, html): \n",
    "        if 'works' in list(html.body.attrs.values())[0]: \n",
    "            content = html.find('div', {'class':'col-md-9 fixed-height'})\n",
    "            try: \n",
    "                author = content.div.h3.text\n",
    "            except: \n",
    "                author = \"\"\n",
    "            try: \n",
    "                title = content.div.h4.text\n",
    "            except: \n",
    "                title = \"\"\n",
    "            stanza_text = content.find_all('p',{'class':'stanza'})\n",
    "            prose_text = content.find_all('p',{'class':'text'})\n",
    "            if len(stanza_text) > len(prose_text): \n",
    "                text = stanza_text\n",
    "            elif len(stanza_text) < len(prose_text): \n",
    "                text = prose_text\n",
    "            Wtext = [x.text.replace('\\n','').strip() for x in text]\n",
    "            metaKeys = [x.text[:-1] for x in html.find('div', {'class':'card-body'}).find_all('h4')]\n",
    "            metaVals = [x.text for x in html.find('div', {'class':'card-body'}).find_all('p')]\n",
    "            metaDict = dict(zip(metaKeys, metaVals))\n",
    "            subDict = {'title': title, \n",
    "                       'text': Wtext}\n",
    "\n",
    "            self.Ws.update(subdict)\n",
    "            #return subDict\n",
    "\n",
    "        elif 'agents' in list(html.body.attrs.values())[0]: \n",
    "            name = html.find('div', {'class': 'card scrollable'}).h2.text\n",
    "            bdate, ddate = html.find('div', {'class': 'card scrollable'}).span.text.split(' - ')\n",
    "            subDict = {'name': name, 'birth': bdate, 'death': ddate}\n",
    "            try: \n",
    "                typeKeys = [x.h4.text for x in html.find_all('div', {'class': 'col-md-4'})]\n",
    "                typeVals = [x.p or x.div.span.text for x in html.find_all('div', {'class': 'col-md-4'})]\n",
    "                typeVals[:2] = [x.text for x in typeVals[:2]]\n",
    "                typeDict = dict(zip(typeKeys, typeVals))\n",
    "                subDict.update(typeDict)\n",
    "            except: \n",
    "                pass\n",
    "\n",
    "            self.As.update(subdict)\n",
    "            #return subDict\n",
    "        \n",
    "        else: \n",
    "            logging.info(f\"Something went wrong while parsing\")\n",
    "            pass\n",
    "    \n",
    "    def parseHTML(self): \n",
    "        threads = min(max_threads, len(self.urls_to_visit)) \n",
    "        print(f\"Parsing with {threads} threads\")\n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=threads) as executor: \n",
    "            executor.map(self.parse_html, self.soups)\n",
    "    \n",
    "    def run(self): \n",
    "        for t, i in list(tables.items())[:2]: \n",
    "            for j in range(0,5): \n",
    "                self.urls_to_visit.append(domain+t+str(j))\n",
    "        \n",
    "        logging.info(f\"Getting As and Ws\")\n",
    "        self.downloadHTML()\n",
    "        logging.info(f\"Done getting As and Ws\")\n",
    "        \n",
    "        logging.info(f\"Parsing As and Ws\")\n",
    "        self.parseHTML()\n",
    "        logging.info(f\"Done parsing As and Ws\")\n",
    "        \n",
    "        logging.info(f\"Making dataframes\")\n",
    "        print(f\"Making AsDf\")\n",
    "        AsDf = pd.DataFrame.from_dict(self.As)\n",
    "        print(f\"Making WsDf\")\n",
    "        WsDf = pd.DataFrame.from_dict(self.Ws)        \n",
    "        logging.info(f\"Done making dataframes\")\n",
    "        \n",
    "        logging.info(f\"Writing to json\")\n",
    "        WsDf.to_json('WsDf.json')\n",
    "        AsDf.to_json('AsDf.json')\n",
    "        logging.info(f\"Done writing to json\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\" = AsDf.append(maprr().get_single(cat='agent', id_num=45))\n",
    "\n",
    "AsDf['birth'] = pd.to_datetime(AsDf['birth'], errors='coerce', infer_datetime_format=True)\n",
    "AsDf['death'] = pd.to_datetime(AsDf['death'], errors='coerce', infer_datetime_format=True)\n",
    "AsDf.sort_index()#[AsDf.name.str.contains('Osip')]\n",
    "AsDf.to_json('AsDf.json')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>birth</th>\n",
       "      <th>death</th>\n",
       "      <th>type_of_agent</th>\n",
       "      <th>sex</th>\n",
       "      <th>occupations</th>\n",
       "      <th>family_social_strata</th>\n",
       "      <th>literary_affiliations</th>\n",
       "      <th>political_affiliations</th>\n",
       "      <th>type_of_corporate_body</th>\n",
       "      <th>affiliation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anna Akhmatova</td>\n",
       "      <td>1889-06-23</td>\n",
       "      <td>1966-03-05</td>\n",
       "      <td>person</td>\n",
       "      <td>female</td>\n",
       "      <td>poet</td>\n",
       "      <td>nobility</td>\n",
       "      <td>Acmeism</td>\n",
       "      <td>independent</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vasilii Dmitrievich Aleksandrovskii</td>\n",
       "      <td>1897-01-15</td>\n",
       "      <td>1934-11-16</td>\n",
       "      <td>person</td>\n",
       "      <td>male</td>\n",
       "      <td>soldier</td>\n",
       "      <td>peasant</td>\n",
       "      <td>Kuznitsa</td>\n",
       "      <td>Bolshevik member</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ivan Nikolaevich Antonov</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>person</td>\n",
       "      <td>male</td>\n",
       "      <td>editor</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>independent</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mikhail Dmitrievich Artamonov</td>\n",
       "      <td>1888-02-22</td>\n",
       "      <td>1958-11-22</td>\n",
       "      <td>person</td>\n",
       "      <td>male</td>\n",
       "      <td>journalist</td>\n",
       "      <td>peasant</td>\n",
       "      <td>Vologda poets</td>\n",
       "      <td>unknown</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nikolai Aseev</td>\n",
       "      <td>1889-07-10</td>\n",
       "      <td>1963-07-16</td>\n",
       "      <td>person</td>\n",
       "      <td>male</td>\n",
       "      <td>soldier</td>\n",
       "      <td>nobility</td>\n",
       "      <td>Left Front of Art: LEF</td>\n",
       "      <td>Bolshevik member</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>Moisei Solomonovich Uritskii</td>\n",
       "      <td>1873-01-14</td>\n",
       "      <td>1918-08-30</td>\n",
       "      <td>person</td>\n",
       "      <td>male</td>\n",
       "      <td>revolutionary</td>\n",
       "      <td>merchant</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Bolshevik member</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>Maximilien Marie Isidore de Robespierre</td>\n",
       "      <td>1758-05-06</td>\n",
       "      <td>1794-06-28</td>\n",
       "      <td>person</td>\n",
       "      <td>male</td>\n",
       "      <td>unknown</td>\n",
       "      <td>professional</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>Iurii Mikhailovich Steklov</td>\n",
       "      <td>1873-08-27</td>\n",
       "      <td>1941-07-15</td>\n",
       "      <td>person</td>\n",
       "      <td>male</td>\n",
       "      <td>revolutionary</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Socialist Revolutionary</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>Christian August Friedrich Peters</td>\n",
       "      <td>1806-09-07</td>\n",
       "      <td>1880-05-08</td>\n",
       "      <td>person</td>\n",
       "      <td>male</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Émile Adolphe Gustave Verhaeren</td>\n",
       "      <td>1855-05-21</td>\n",
       "      <td>1916-11-27</td>\n",
       "      <td>person</td>\n",
       "      <td>male</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>295 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        name      birth      death  \\\n",
       "1                             Anna Akhmatova 1889-06-23 1966-03-05   \n",
       "2        Vasilii Dmitrievich Aleksandrovskii 1897-01-15 1934-11-16   \n",
       "3                   Ivan Nikolaevich Antonov        NaT        NaT   \n",
       "4              Mikhail Dmitrievich Artamonov 1888-02-22 1958-11-22   \n",
       "5                              Nikolai Aseev 1889-07-10 1963-07-16   \n",
       "..                                       ...        ...        ...   \n",
       "300             Moisei Solomonovich Uritskii 1873-01-14 1918-08-30   \n",
       "301  Maximilien Marie Isidore de Robespierre 1758-05-06 1794-06-28   \n",
       "302               Iurii Mikhailovich Steklov 1873-08-27 1941-07-15   \n",
       "303        Christian August Friedrich Peters 1806-09-07 1880-05-08   \n",
       "304          Émile Adolphe Gustave Verhaeren 1855-05-21 1916-11-27   \n",
       "\n",
       "    type_of_agent     sex    occupations family_social_strata  \\\n",
       "1          person  female           poet             nobility   \n",
       "2          person    male        soldier              peasant   \n",
       "3          person    male         editor              unknown   \n",
       "4          person    male     journalist              peasant   \n",
       "5          person    male        soldier             nobility   \n",
       "..            ...     ...            ...                  ...   \n",
       "300        person    male  revolutionary             merchant   \n",
       "301        person    male        unknown         professional   \n",
       "302        person    male  revolutionary              unknown   \n",
       "303        person    male        unknown              unknown   \n",
       "304        person    male        unknown              unknown   \n",
       "\n",
       "      literary_affiliations   political_affiliations type_of_corporate_body  \\\n",
       "1                   Acmeism              independent                   None   \n",
       "2                  Kuznitsa         Bolshevik member                   None   \n",
       "3                   unknown              independent                   None   \n",
       "4             Vologda poets                  unknown                   None   \n",
       "5    Left Front of Art: LEF         Bolshevik member                   None   \n",
       "..                      ...                      ...                    ...   \n",
       "300                 unknown         Bolshevik member                   None   \n",
       "301                 unknown                  unknown                   None   \n",
       "302                 unknown  Socialist Revolutionary                   None   \n",
       "303                 unknown                  unknown                   None   \n",
       "304                 unknown                  unknown                   None   \n",
       "\n",
       "    affiliation  \n",
       "1          None  \n",
       "2          None  \n",
       "3          None  \n",
       "4          None  \n",
       "5          None  \n",
       "..          ...  \n",
       "300        None  \n",
       "301        None  \n",
       "302        None  \n",
       "303        None  \n",
       "304        None  \n",
       "\n",
       "[295 rows x 11 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AsDf = pd.read_json('AsDf.json')\n",
    "AsDf['birth'] = pd.to_datetime(AsDf['birth'], errors='coerce', infer_datetime_format=True)\n",
    "AsDf['death'] = pd.to_datetime(AsDf['death'], errors='coerce', infer_datetime_format=True)\n",
    "AsDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### libDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(586, 12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "libDf = pd.read_json('WsDf.json')\n",
    "libDf.index.name = 'w_id'\n",
    "libDf.columns = lib_cols\n",
    "libDf['comp_date'] = pd.to_datetime(libDf['comp_date'], errors='coerce')\n",
    "libDf['pub_year'] = pd.to_datetime(libDf['pub_year'], errors='coerce')\n",
    "libDf['pub_year'] = libDf.pub_year.apply(lambda x: x.year).astype('int64', errors='ignore')\n",
    "print(libDf.shape)\n",
    "#type(libDf.loc[1, 'pub_year'])\n",
    "libDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### authorsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'num_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-0e9eecde7ead>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mauthorsDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'author'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'num_works'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mauthorsDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_words'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'author'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mauthorsDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'avg_wpw'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthorsDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mauthorsDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_works\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mauthorsDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauthorsDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'avg_wpw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'author'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mauthorsDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAsDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthorsDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'right'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index_x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5463\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5464\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5465\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'num_words'"
     ]
    }
   ],
   "source": [
    "authorsDf = libDf.reset_index().groupby('author').size().to_frame().rename(columns={0:'num_works'})\n",
    "authorsDf['num_words'] = libDf.reset_index().groupby('author').sum().num_words\n",
    "authorsDf['avg_wpw'] = round(authorsDf.num_words/authorsDf.num_works, 2)\n",
    "authorsDf = authorsDf.reset_index().sort_values(by=['avg_wpw'], ascending=False).rename(columns={'author':'name'})\n",
    "authorsDf = pd.merge(AsDf.reset_index(), authorsDf.reset_index(), how='right', on='name').set_index('index_x')\n",
    "authorsDf.index.name = 'a_id'\n",
    "authorsDf.to_json('authorsDf.json', date_format='iso')\n",
    "authorsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>birth</th>\n",
       "      <th>death</th>\n",
       "      <th>type_of_agent</th>\n",
       "      <th>sex</th>\n",
       "      <th>occupations</th>\n",
       "      <th>family_social_strata</th>\n",
       "      <th>literary_affiliations</th>\n",
       "      <th>political_affiliations</th>\n",
       "      <th>type_of_corporate_body</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>index_y</th>\n",
       "      <th>num_works</th>\n",
       "      <th>num_words</th>\n",
       "      <th>avg_wpw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Aleksandr Vasil'evich Chaianov</td>\n",
       "      <td>1888-01-29T00:00:00.000Z</td>\n",
       "      <td>1937-10-03T00:00:00.000Z</td>\n",
       "      <td>person</td>\n",
       "      <td>male</td>\n",
       "      <td>writer</td>\n",
       "      <td>merchant</td>\n",
       "      <td>unknown</td>\n",
       "      <td>agricultural cooperativist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4591</td>\n",
       "      <td>4591.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Mikhail Mikhailovich Prishvin</td>\n",
       "      <td>1873-02-04T00:00:00.000Z</td>\n",
       "      <td>1954-01-16T00:00:00.000Z</td>\n",
       "      <td>person</td>\n",
       "      <td>male</td>\n",
       "      <td>writer</td>\n",
       "      <td>merchant</td>\n",
       "      <td>unknown</td>\n",
       "      <td>nationalist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>3067</td>\n",
       "      <td>3067.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Aleksandra Kollontai</td>\n",
       "      <td>1872-03-31T00:00:00.000Z</td>\n",
       "      <td>1952-03-09T00:00:00.000Z</td>\n",
       "      <td>person</td>\n",
       "      <td>female</td>\n",
       "      <td>activist</td>\n",
       "      <td>nobility</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Menshevik</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1082</td>\n",
       "      <td>1082.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>S. D. Spasskii</td>\n",
       "      <td>1898-12-21T00:00:00.000Z</td>\n",
       "      <td>1956-08-24T00:00:00.000Z</td>\n",
       "      <td>person</td>\n",
       "      <td>male</td>\n",
       "      <td>writer</td>\n",
       "      <td>professional</td>\n",
       "      <td>Futurism</td>\n",
       "      <td>independent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>826</td>\n",
       "      <td>826.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Evgenii Nikolaevich Chirikov</td>\n",
       "      <td>1864-08-05T00:00:00.000Z</td>\n",
       "      <td>1932-01-18T00:00:00.000Z</td>\n",
       "      <td>person</td>\n",
       "      <td>male</td>\n",
       "      <td>writer</td>\n",
       "      <td>nobility</td>\n",
       "      <td>Gor'kii circle</td>\n",
       "      <td>Populist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>3064</td>\n",
       "      <td>766.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mikhail Dmitrievich Artamonov</td>\n",
       "      <td>1888-02-22T00:00:00.000Z</td>\n",
       "      <td>1958-11-22T00:00:00.000Z</td>\n",
       "      <td>person</td>\n",
       "      <td>male</td>\n",
       "      <td>journalist</td>\n",
       "      <td>peasant</td>\n",
       "      <td>Vologda poets</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43</td>\n",
       "      <td>9</td>\n",
       "      <td>387</td>\n",
       "      <td>43.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Sergei Stradnyi</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>person</td>\n",
       "      <td>male</td>\n",
       "      <td>poet</td>\n",
       "      <td>peasant</td>\n",
       "      <td>Smolensk Proletkult</td>\n",
       "      <td>Proletkult Bolshevik</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63</td>\n",
       "      <td>7</td>\n",
       "      <td>300</td>\n",
       "      <td>42.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Pavel Leonidovich Daletskii</td>\n",
       "      <td>1898-02-02T00:00:00.000Z</td>\n",
       "      <td>1963-07-08T00:00:00.000Z</td>\n",
       "      <td>person</td>\n",
       "      <td>male</td>\n",
       "      <td>poet</td>\n",
       "      <td>professional</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53</td>\n",
       "      <td>3</td>\n",
       "      <td>124</td>\n",
       "      <td>41.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Boris Virganskii</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>person</td>\n",
       "      <td>male</td>\n",
       "      <td>poet</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Bolshevik-leaning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>207</td>\n",
       "      <td>34.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>David Davidovich Burliuk</td>\n",
       "      <td>1882-07-21T00:00:00.000Z</td>\n",
       "      <td>1967-01-15T00:00:00.000Z</td>\n",
       "      <td>person</td>\n",
       "      <td>male</td>\n",
       "      <td>poet</td>\n",
       "      <td>professional</td>\n",
       "      <td>Futurism</td>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              name                     birth  \\\n",
       "16  Aleksandr Vasil'evich Chaianov  1888-01-29T00:00:00.000Z   \n",
       "57   Mikhail Mikhailovich Prishvin  1873-02-04T00:00:00.000Z   \n",
       "40            Aleksandra Kollontai  1872-03-31T00:00:00.000Z   \n",
       "65                  S. D. Spasskii  1898-12-21T00:00:00.000Z   \n",
       "17    Evgenii Nikolaevich Chirikov  1864-08-05T00:00:00.000Z   \n",
       "..                             ...                       ...   \n",
       "4    Mikhail Dmitrievich Artamonov  1888-02-22T00:00:00.000Z   \n",
       "66                 Sergei Stradnyi                      None   \n",
       "19     Pavel Leonidovich Daletskii  1898-02-02T00:00:00.000Z   \n",
       "70                Boris Virganskii                      None   \n",
       "15        David Davidovich Burliuk  1882-07-21T00:00:00.000Z   \n",
       "\n",
       "                       death type_of_agent     sex occupations  \\\n",
       "16  1937-10-03T00:00:00.000Z        person    male      writer   \n",
       "57  1954-01-16T00:00:00.000Z        person    male      writer   \n",
       "40  1952-03-09T00:00:00.000Z        person  female    activist   \n",
       "65  1956-08-24T00:00:00.000Z        person    male      writer   \n",
       "17  1932-01-18T00:00:00.000Z        person    male      writer   \n",
       "..                       ...           ...     ...         ...   \n",
       "4   1958-11-22T00:00:00.000Z        person    male  journalist   \n",
       "66                      None        person    male        poet   \n",
       "19  1963-07-08T00:00:00.000Z        person    male        poet   \n",
       "70                      None        person    male        poet   \n",
       "15  1967-01-15T00:00:00.000Z        person    male        poet   \n",
       "\n",
       "   family_social_strata literary_affiliations      political_affiliations  \\\n",
       "16             merchant               unknown  agricultural cooperativist   \n",
       "57             merchant               unknown                 nationalist   \n",
       "40             nobility               unknown                   Menshevik   \n",
       "65         professional              Futurism                 independent   \n",
       "17             nobility        Gor'kii circle                    Populist   \n",
       "..                  ...                   ...                         ...   \n",
       "4               peasant         Vologda poets                     unknown   \n",
       "66              peasant   Smolensk Proletkult        Proletkult Bolshevik   \n",
       "19         professional               unknown                     unknown   \n",
       "70              unknown               unknown           Bolshevik-leaning   \n",
       "15         professional              Futurism                     unknown   \n",
       "\n",
       "    type_of_corporate_body  affiliation  index_y  num_works  num_words  \\\n",
       "16                     NaN          NaN        8          1       4591   \n",
       "57                     NaN          NaN       44          1       3067   \n",
       "40                     NaN          NaN        9          1       1082   \n",
       "65                     NaN          NaN       56          1        826   \n",
       "17                     NaN          NaN       25          4       3064   \n",
       "..                     ...          ...      ...        ...        ...   \n",
       "4                      NaN          NaN       43          9        387   \n",
       "66                     NaN          NaN       63          7        300   \n",
       "19                     NaN          NaN       53          3        124   \n",
       "70                     NaN          NaN       20          6        207   \n",
       "15                     NaN          NaN       21          1         32   \n",
       "\n",
       "    avg_wpw  \n",
       "16  4591.00  \n",
       "57  3067.00  \n",
       "40  1082.00  \n",
       "65   826.00  \n",
       "17   766.00  \n",
       "..      ...  \n",
       "4     43.00  \n",
       "66    42.86  \n",
       "19    41.33  \n",
       "70    34.50  \n",
       "15    32.00  \n",
       "\n",
       "[72 rows x 15 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authorsDf = pd.read_json('authorsDf.json')\n",
    "#authorsDf[['birth', 'death']] = authorsDf[['birth', 'death']].apply(pd.to_datetime, format=\"%Y-%m-%d\")\n",
    "#[dt.to_datetime().date() for dt in authorsDf[['birth', 'death']]]\n",
    "authorsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(authorsDf.loc[15, 'death'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### worksDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-101-08ebebde4aa9>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  textsDf['num_words'] = libDf['num_words'] = textsDf.text.apply(lambda k: len([a for b in [x.split() for y in k for x in y.split('               ')] for a in b if a.isalpha() == True]))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>[в которой благосклонный читатель знакомится с...</td>\n",
       "      <td>4591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>[Случалось, на огонек во время перелета, или в...</td>\n",
       "      <td>3067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>[Так как Волга была великой исторической дорог...</td>\n",
       "      <td>1616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>[Вы, которым шестьдесят лет, или даже вы, кото...</td>\n",
       "      <td>1570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>[Волга! Одна из значительнейших рек всего земн...</td>\n",
       "      <td>1234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>[Ветер забил с Балта.               Мглой упил...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>[Белогвардейцы! Гордиев узел                  ...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>[Под рокот гражданских бурь,               В л...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>[Пусть стихи мои развеют                    Ва...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>[...Сын казака, казак...               Так нач...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>586 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  num_words\n",
       "w_id                                                              \n",
       "140   [в которой благосклонный читатель знакомится с...       4591\n",
       "403   [Случалось, на огонек во время перелета, или в...       3067\n",
       "551   [Так как Волга была великой исторической дорог...       1616\n",
       "61    [Вы, которым шестьдесят лет, или даже вы, кото...       1570\n",
       "142   [Волга! Одна из значительнейших рек всего земн...       1234\n",
       "...                                                 ...        ...\n",
       "523   [Ветер забил с Балта.               Мглой упил...         17\n",
       "479   [Белогвардейцы! Гордиев узел                  ...         14\n",
       "482   [Под рокот гражданских бурь,               В л...         14\n",
       "400   [Пусть стихи мои развеют                    Ва...         13\n",
       "464   [...Сын казака, казак...               Так нач...         12\n",
       "\n",
       "[586 rows x 2 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textsDf = libDf[['text']]\n",
    "textsDf['num_words'] = libDf['num_words'] = textsDf.text.apply(lambda k: len([a for b in [x.split() for y in k for x in y.split('               ')] for a in b if a.isalpha() == True]))\n",
    "textsDf.sort_values('num_words', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['title', 'genre', 'year', 'num_lps'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-93ab617949ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mworksDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'author'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'genre'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'num_lps'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'num_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mworksDf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3028\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3029\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3030\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3032\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['title', 'genre', 'year', 'num_lps'] not in index\""
     ]
    }
   ],
   "source": [
    "worksDf = libDf[['title','year','author','genre','num_lps','num_words']]\n",
    "worksDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-a8deca6d81cf>:2: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  lpDf = lpDf.text.apply(lambda x: pd.Series([y for y in x])).stack().to_frame().rename(columns={0:'lp_str'})\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'OHCO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-a8deca6d81cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlpDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlpDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlpDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'lp_str'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlpDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOHCO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlpDf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlpDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlp_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.rename(columns={0:'token'})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OHCO' is not defined"
     ]
    }
   ],
   "source": [
    "lpDf = libDf[['text']]\n",
    "lpDf = lpDf.text.apply(lambda x: pd.Series([y for y in x])).stack().to_frame().rename(columns={0:'lp_str'})\n",
    "lpDf.index.names = OHCO[:2]\n",
    "lpDf\n",
    "tokenDf = lpDf.lp_str.apply(lambda x: tokenize(x)).to_frame()#.rename(columns={0:'token'})\n",
    "#tokenDf = lpDf.lp_str.apply(lambda x: y.text for y in tokenize(x)[1])\n",
    "tokenDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tokenize(lpDf.lp_str): \n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
